---
title: "ILAA Tutorial"
author: "Jose Tamez"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    fig_caption: yes
    number_sections: yes
  word_document: 
    reference_docx: WordStyle_FRESA.docx
    toc: yes
    fig_caption: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(collapse = TRUE, warning = FALSE, message = FALSE,comment = "#>")

pander::panderOptions('digits', 3)
pander::panderOptions('keep.trailing.zeros',TRUE)
op <- par(no.readonly = TRUE)

```

# Introduction

Iterative Linear Association Analysis (ILAA) is a computational method
that creates a linear transformation from a sample of multidimensional
data that effectively removes linear associations between data
variables. The returned transformation matrix can be used to:

1.  Do an exploratory analysis of latent variables and their association
    to all the observed variables

2.  Do exploratory discovery of latent variables associated with an
    specific outcome-target

3.  Addressing multicollinearity issues in linear regression models

    1.  Better estimation and interpretation of model variables

    2.  Improve linear model performance

4.  Simplify the multidimensional search space for many ML algorithms

The objective of this tutorial is to guide users in using the ILAA to
effectively accomplish the aforementioned tasks. The tutorial will
showcase:

-   Transform a data frame affected by data multicollinearity into a new
    a data frame with minimum data correlation among variables

-   Visualize the transformation matrix

-   Explore the returned formulas for each one of the returned latent
    variables

-   Understand and interpret the returned latent variables

-   Use ILAA as a pre-processing step to model a specific target outcome
    using linear models

    -   Explore the model in the transformed space
    -   Get the observed variables coefficients.

## The Libraries

ILAA is a wrapper of the more general method of data decorrelation
algorithm (IDeA) implemented in R, and both are part of the FRESA.CAD
3.4.6 package.

```{r}
## From git hub
#install_github("joseTamezPena/FRESA.CAD")

## For ILAA
library("FRESA.CAD")

## For network analysis
library(igraph)

```

# Material and Methods

For this tutorial I'll use the body-fat prediction data set. The data
was downloaded from Kaggle:

<https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset>

The Kaggle data disclaimer:

"Source The data were generously supplied by Dr. A. Garth Fisher who
gave permission to freely distribute the data and use for non-commercial
purposes.

Roger W. Johnson Department of Mathematics & Computer Science South
Dakota School of Mines & Technology 501 East St. Joseph Street Rapid
City, SD 57701

email address:
[rwjohnso\@silver.sdsmt.edu](mailto:rwjohnso@silver.sdsmt.edu){.email}
web address: <http://silver.sdsmt.edu/~rwjohnso>"

## Loading the Data

The following code snippet loads the data and removes the density
information from the data. It also computes the Body Mass Index (BMI)

```{r}
body_fat <- read.csv("~/GitHub/LatentBiomarkers/Data/BodyFat/BodyFat.csv", header=TRUE)

### Removing density as estimator
body_fat$Density <- NULL

body_fat$BMI <- 10000*body_fat$Weight*0.453592/((body_fat$Height*2.54)^2)
## Removing subjects with data errors
body_fat <- body_fat[body_fat$BMI<=50,]

```

## ILAA Unsupervised Processing

The ILAA function is:

``` r
 ILAA(data=NULL,
                thr=0.80,
                method=c("pearson","spearman"),
                Outcome=NULL,
                drivingFeatures=NULL,
                maxLoops=100,
                verbose=FALSE
      )
```

where:

-   `data`: The source data-frame

-   *`thr`* : The target correlation goal.

-   *`method`* : Defines the correlation measure

-   `Outcome` The name of the target variable, and it is required for
    supervised learning

-   `drivingFeatures` : Defines a set of variables that are aimed to be
    basis unaltered vectors

-   `maxLoops` : The maximum number of iterations cycles

-   `verbose` : Display the evolution of the algorithm.

By default, the ILAA function will target a correlation lower than 0.8
using the Pearson correlation measure. But user has the freedom to chose
between robust fitting with Spearman correlation measure, and/or set the
level of feature association by lowering the threshold. The following
snippet shows the different options.

```{r results = "asis", warning = FALSE}

# Default call
body_fat_Decorrelated <- ILAA(body_fat)
pander::pander(colnames(attr(body_fat_Decorrelated,"UPLTM")))

# Explore the convergence metrics in verbose mode
body_fat_Decorrelated <- ILAA(body_fat,verbose=TRUE)
pander::pander(colnames(attr(body_fat_Decorrelated,"UPLTM")))

# Robust Linear Fitting with spearman correlation measure
body_fat_Decorrelated <- ILAA(body_fat,method="spearman",verbose=TRUE)
pander::pander(colnames(attr(body_fat_Decorrelated,"UPLTM")))

# Lowering the threshold
body_fat_Decorrelated <- ILAA(body_fat,thr=0.4,verbose=TRUE)
pander::pander(colnames(attr(body_fat_Decorrelated,"UPLTM")))

# Tring to achive the maximum independence beteeen variables, i.e., thr=0.0
body_fat_Decorrelated <- ILAA(body_fat,thr=0.0,verbose=TRUE)
pander::pander(colnames(attr(body_fat_Decorrelated,"UPLTM")))

```

I'll set the correlation goal to 0.2 in verbose mode. Then I'll continue
the tutorial using this output.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

# Calling ILAA to achieve a final correlation of 0.2
body_fat_Decorrelated <- ILAA(body_fat,thr=0.2,verbose=TRUE)

```

### Data Frame Attributes

The returned data matrix contains the following attributes

``` r
  attr(body_fat_Decorrelated,"UPLTM")            #The transformation matrix
  attr(body_fat_Decorrelated,"fscore")           #The score of each feature
  attr(body_fat_Decorrelated,"drivingFeatures")  #The list of driving features
  attr(body_fat_Decorrelated,"unaltered")        #The list of unaltered features
  attr(body_fat_Decorrelated,"LatentVariables")  #The list of latent variables
  attr(body_fat_Decorrelated,"R.critical")       #The estimated minium correlation
  attr(body_fat_Decorrelated,"IDeAEvolution")    #Evolution of the algorithm
```

The main attributes is "`UPLTM`". That stores the specific linear
transformation matrix from observed variables to the latent variable.
The "`IDeAEvolution`" attribute can be used to verify if the algorithm
achieved the target correlation goal, and the sparsity of the returned
matrix.

### Plotting the Evolution

Here we will use the `attr(dataTransformed,"IDeAEvolution")` to plot the
evolution of the correlation measure and the evolution of the matrix
sparsity.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 3.5, fig.width= 7.0}
par(mfrow=c(1,2),cex=0.5)

# Correlation
yval <- attr(body_fat_Decorrelated,"IDeAEvolution")$Corr
xidx <- c(1:length(yval))
plot(xidx,yval,
     xlab="Iteration Cycle",
     ylab="Max. Pearson Correlation",
     ylim=c(0,1.0),
     main="Evolution of the maximum Correlation")
  lfit <-try(loess(yval~xidx,span=0.5));
  if (!inherits(lfit,"try-error"))
  {
    plx <- try(predict(lfit,se=TRUE))
    if (!inherits(plx,"try-error"))
    {
      lines(xidx,plx$fit,lty=1,col="red")
    }
  }

# Sparsity  
yval <- attr(body_fat_Decorrelated,"IDeAEvolution")$Spar

plot(xidx,yval,
     xlab="Iteration Cycle",
     ylab="Matrix Sparcity",
     ylim=c(0,1.0),
     main="Evolution of the Matrix Sparcity")
  lfit <-try(loess(yval~xidx,span=0.5));
  if (!inherits(lfit,"try-error"))
  {
    plx <- try(predict(lfit,se=TRUE))
    if (!inherits(plx,"try-error"))
    {
      lines(xidx,plx$fit,lty=1,col="red")
    }
  }

```

### The ILAA Transformed Data

Before exploring into more detail, the properties of the `ILAA` results.
Let us first verify that the returned matrix does not contain features
with very high correlation among them.

Here I'll plot the original correlation and the correlation of the
returned data set.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

# The original
  par(cex=0.6,cex.main=0.85,cex.axis=0.7)
  cormat <- cor(body_fat,method="pearson")
  gplots::heatmap.2(abs(cormat),
                    trace = "none",
                    mar = c(5,5),
                    col=rev(heat.colors(5)),
                    main = "Original Correlation",
                    cexRow = 0.75,
                    cexCol = 0.75,
                     srtCol=30,
                     srtRow=60,
                    key.title=NA,
                    key.xlab="|Pearson Correlation|",
                    xlab="Feature", ylab="Feature")

# The transformed
  cormat <- cor(body_fat_Decorrelated,method="pearson")
  gplots::heatmap.2(abs(cormat),
                    trace = "none",
                    mar = c(5,5),
                    col=rev(heat.colors(5)),
                    main = "Correlation After ILAA",
                    cexRow = 0.75,
                    cexCol = 0.75,
                     srtCol=30,
                     srtRow=60,
                    key.title=NA,
                    key.xlab="|Pearson Correlation|",
                    xlab="Feature", ylab="Feature")

```

### Exploring the Transformation

The `attr(body_fat_Decorrelated,"UPLTM")` returns the transformation
matrix. The `UPLTM` is sparse, here I show a heat map of the
transformation matrix that shows which elements are different from zero.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

  UPLTM <- attr(body_fat_Decorrelated,"UPLTM")
  
  gplots::heatmap.2(1.0*(abs(UPLTM)>0),
                    trace = "none",
                    mar = c(5,5),
                    col=rev(heat.colors(5)),
                    Rowv=NULL,
                    Colv="Rowv",
                    dendrogram="none",
                    main = "Transformation matrix",
                    cexRow = 0.75,
                    cexCol = 0.75,
                   srtCol=30,
                   srtRow=60,
                    key.title=NA,
                    key.xlab="|Beta|>0",
                    xlab="Output Feature", ylab="Input Feature")
  
```

### The Latent Formulas

The sparsity of the `UPLTM` matrix can be analyzed to get the formula
for each one of the latent formulas. The `getLatentCoefficients()` and
its attribute: `attr(LatentFormulas,"LatentCharFormulas")` can be used
to display the formula of the latent variables.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}
# Get a list with the latent formulas' coefficients
LatentFormulas <- getLatentCoefficients(body_fat_Decorrelated)

# A string character with the formulas can be obtained by:
charFormulas <- attr(LatentFormulas,"LatentCharFormulas")
pander::pander(as.matrix(charFormulas))

```

### The Formula Network

The `graph_from_adjacency_matrix()` function from `igraph` can be used
to visualize the association between variables.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}
par(op)

transform <- log(abs(attr(body_fat_Decorrelated,"UPLTM"))+1.0)
colnames(transform) <- str_remove_all(colnames(transform),"La_")


VertexSize <- apply(transform,2,mean)
VertexSize <- 10*VertexSize/max(VertexSize)


gr <- graph_from_adjacency_matrix(transform,mode = "directed",diag = FALSE,weighted=TRUE)
gr$layout <- layout_with_fr

fc <- cluster_optimal(gr)
plot(fc, gr,
     edge.width=E(gr)$weight,
     edge.arrow.size=0.5,
     edge.arrow.width=0.5,
     vertex.size=VertexSize,
     vertex.label.cex=0.85,
     vertex.label.dist=2,
     main="Feature Association")
par(op)


```

### ILAA Solution is Data Dependent

I'll generate 100 solutions of the UPLTM and aggregate the non-zero coefficients.
Then, I'll plot the graph followed by the heat map of the frequency of hits

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}
par(op)

dsize <- nrow(body_fat);
taccmatrix <- cor(body_fat)*0;
for (lp in c(1:100))
{
  dmat <- ILAA(body_fat[sample(dsize,0.8*dsize),],thr=0.2)
  transform <- attr(dmat,"UPLTM") !=0 
  colnames(transform) <- str_remove_all(colnames(transform),"La_")
  taccmatrix[,colnames(transform)] <- taccmatrix[,colnames(transform)] + transform
}
taccmatrix <- taccmatrix/100;
VertexSize <- apply(taccmatrix,2,mean)
VertexSize <- 10*VertexSize/max(VertexSize)


gr <- graph_from_adjacency_matrix(taccmatrix,mode = "directed",diag = FALSE,weighted=TRUE)
gr$layout <- layout_with_fr

fc <- cluster_optimal(gr)
plot(fc, gr,
     edge.width=E(gr)$weight,
     edge.arrow.size=0.5,
     edge.arrow.width=0.5,
     vertex.size=VertexSize,
     vertex.label.cex=0.85,
     vertex.label.dist=2.0,
     main="Feature Association")
par(op)

  gplots::heatmap.2(taccmatrix,
                    trace = "none",
                    mar = c(5,5),
                    Rowv=NULL,
                    Colv="Rowv",
                    dendrogram="none",
                    col=rev(heat.colors(5)),
                    main = "Hit Percentage",
                    cexRow = 0.75,
                    cexCol = 0.75,
                   srtCol=30,
                   srtRow=60,
                    key.title=NA,
                    key.xlab="|Beta|>0",
                    xlab="Output Feature", ylab="Input Feature")
  
```

### Latent Variable Interpretation

The ILAA returns the Unit Preserving Linear Transformation Matrix
(UPLTM). This specific transformation is the combination of
statistically significant linear association analysis between feature
pairs. Each significant association is modeled by a linear equation;
henceforth, the interpretation of each feature is as follows:

-   Each discovered latent variable is the residual of the observed
    parent variable *vs.* the suitable model of the variables associated
    with the parent variable. For example: $$
    Lawrist = Wrist + 0.017BodyFat - 0.026Weight.  
    $$

    Describes that the $Wrist$ is associated with the $BodyFat$ and the
    $Weight$, and the latent variable $Lawrist$ is the amount of
    information in the $Wrist$ not found by $BodyFat$ nor the $Weight$.

-   The model of the $Wrist$ is therefore:

$$
Wrist = -0.017BodyFat + 0.026Weight.  
$$

The following code shows the association of the latent variable to each
one of the observed parent variable, and the association of the parent
variables to its linear model.

```{r warning = FALSE, dpi=300, fig.height= 3.5, fig.width= 7.0}

par(mfrow=c(1,2),cex=0.35)
fnames <- names(charFormulas)[1]
for (fnames in names(charFormulas))
{
  obsname <- str_remove(fnames,"La_")
  menv <- mean(body_fat_Decorrelated[,fnames])
  range <- max(body_fat[,obsname])-min(body_fat[,obsname])
  ylim <- c(menv-range/2,menv+range/2)
  plot(body_fat[,obsname],
       body_fat_Decorrelated[,fnames],
       ylim=ylim,
       ylab=fnames,
       xlab=obsname,
       main=paste("ILAA Latent Variable:",fnames))
  
  deformula <- LatentFormulas[[fnames]]
  noInames <- names(deformula)[names(deformula) != obsname]
  predObs <- -(as.matrix(body_fat[,noInames]) %*% deformula[noInames])
  plot(predObs,
       body_fat[,obsname],
       ylab=obsname,
       xlab=charFormulas[fnames],
       main=paste("ILAA Generated Predictions of",obsname),
       cex.labels=0.5)
}

par(op)

```

The visual inspection of the above-displayed figures shows that some
latent variables are not associated with the original parent variable,
but their model is fully correlated to the observed parent variable. A
clear example is BMI (The last plot in the above figure).

#ILAA for Supervised Learning

The rerecorded use of ILAA transformation in supervised learning is to
split the data into training and validation sets. Henceforth, the next
lines of code will split the data into training (75%) and testing (25%)

## Split into Training Testing Sets

```{r}

# 75% for training 25% for testing 
set.seed(2)
trainsamples <- sample(nrow(body_fat),3*nrow(body_fat)/4)

trainingset <- body_fat[trainsamples,]
testingset <- body_fat[-trainsamples,]

```

## Data Train Analysis and Prediction of the Test Set

By default, `ILAA()` transforms are blind to outcome associations. but
in supervised learning the user is free to specify a target outcome to
drive the shape of the transformation matrix. Outcome-driven
transformations try to keep unaltered features strongly associated with
the target.

The `predictDecorrelate()` function can be used to predict any new
dataset from an `ILAA` transformed object.

The next code snippet shows the process of transforming the training set
and then using the returned object to transform the testing set using
both outcome-blind and outcome-driven transformations.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

## Outcome-blind
body_fat_Decorrelated_train <- ILAA(trainingset,
                                    thr=0.2,
                                    Outcome="BodyFat",
                                    verbose=TRUE)
pander::pander(attr(body_fat_Decorrelated_train,"drivingFeatures"))

body_fat_Decorrelated_test <- predictDecorrelate(body_fat_Decorrelated_train
                                                 ,testingset)

## Outcome-driven transformation
body_fat_Decorrelated_trainD <- ILAA(trainingset,
                                     thr=0.2,
                                     Outcome="BodyFat",
                                     drivingFeatures="BodyFat",
                                     verbose=TRUE)

pander::pander(attr(body_fat_Decorrelated_trainD,"drivingFeatures"))

body_fat_Decorrelated_testD <- predictDecorrelate(body_fat_Decorrelated_trainD
                                                  ,testingset)

```

### Train a Regression Model for Body Fat Prediction

Once we have a transformed training and testing set, we can proceed to
train a linear model of the body fat content. For this example we will
use the `LASSO_1SE()` function of the FRESA.CAD package to model the
$BodyFat$ using all the variables in the transformed training set.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

## Outcome-Blind
modelBodyFat <- LASSO_1SE(BodyFat~.,body_fat_Decorrelated_train)
pander::pander(as.matrix(modelBodyFat$coef))

## Outcome-Driven
modelBodyFatD <- LASSO_1SE(BodyFat~.,body_fat_Decorrelated_trainD)
pander::pander(as.matrix(modelBodyFatD$coef))


```

The last lines of code display the beta coefficients of the model.

### The Model Coefficients in the Observed Space

The FRESA.CAD package provides a handy function, `getObservedCoef()`m to
get the linear beta coefficients from the transformed object. The next
code shows the procedure.

```{r  results = "asis", warning = FALSE}

# Get the coefficients in the observed space for the outcome-blind
observedCoef <- getObservedCoef(body_fat_Decorrelated_train,modelBodyFat)
pander::pander(as.matrix(observedCoef$coefficients))

# The outcome-driven coefficients
observedCoefD <- getObservedCoef(body_fat_Decorrelated_trainD,modelBodyFatD)
pander::pander(as.matrix(observedCoefD$coefficients))

```

### Predict Using the Transformed Data-Set

The user can predict the BodyFat content using the handy `predict()`
function. After that we can measure the testing performance using the
`predictionStats_regression()` function.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

## OUtcome-Blind 
predicBodyFat <- predict(modelBodyFat,body_fat_Decorrelated_test)
rmetrics <- predictionStats_regression(cbind(testingset$BodyFat,
                                             predicBodyFat),
                                       "Body Fat: Blind")
pander::pander(rmetrics)

## Outcome-Driven
predicBodyFatD <- predict(modelBodyFatD,body_fat_Decorrelated_testD)
rmetrics <- predictionStats_regression(cbind(testingset$BodyFat,
                                             predicBodyFatD),
                                       "Body Fat: Driven")
pander::pander(rmetrics)

```

The reported metrics indicated that the model predictions are highly
correlated to the real $BodyFat$

### Prediction Using the Observed Features

An ILAA user has the option to predict the $BodyFat$ content from the
observed testing set using the computed beta coefficients. The next
lines of code show how to do the prediction using `model.matrix()` R
function and the dot product `%*%` :

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}


predicBodyFatObst <- model.matrix(formula(observedCoef$formula),testingset) %*% observedCoef$coefficients

plot(predicBodyFatObst,
     predicBodyFat,
     xlab="Observed Space",
     ylab="Transformed Space",
     main="Test Predictions: Observed vs. Transformed")

```

The last plot shows the expected result: that both predictions are
identical.

### Comparison to Raw Model

A last experiment is to compare the differences between a LASSO model
created from the observed features to the model created from the
transformed observations.

The next lines of code compute the linear model using LASSO from the
original observed data. Then, it computes the predicted performance.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}
rawmodelBodyFat <- LASSO_1SE(BodyFat~.,trainingset)
pander::pander(rawmodelBodyFat$coef)

rawpredicBodyFat <- predict(rawmodelBodyFat,testingset)
rmetrics <- predictionStats_regression(cbind(testingset$BodyFat,
                                             rawpredicBodyFat),"Body Fat")
pander::pander(rmetrics)

```

The evaluation of the testing results indicates that the observed model
predictions have a correlation of 0.875. Slightly superior, but not
statistically significant, to the one observed from the model estimated
from the transformed space: ( $\rho _t=0.863$ vs. $\rho _o=0.875$ )

### Comparing the Feature Significance on the Models

The main advantage of the ILAA transformation is that the returned
latent variables are not colinear hence the statistical significance of
the beta coefficients are not affected by multicolinearity. The next
code snippet shows how to get the beta coefficients using the `lm()` ,
and `summary.lm()` functions.

The inspection of the summary results clearly shows that most of the
beta coefficients on the transformed data set are significant.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

## Raw Model
par(mfrow=c(2,2),cex=0.5)
rawlm <- lm(BodyFat~.,
            trainingset[,c("BodyFat",names(rawmodelBodyFat$coef)[-1])])
pander::pander(rawlm,add.significance.stars=TRUE)
plot(rawlm)

## Outcome-Blind
par(mfrow=c(2,2),cex=0.5)
Delm <- lm(BodyFat~.,body_fat_Decorrelated_train[,c("BodyFat",names(modelBodyFat$coef)[-1])])
pander::pander(Delm,add.significance.stars=TRUE)
plot(Delm)

## Outcome-Driven
par(mfrow=c(2,2),cex=0.5)
Delm <- lm(BodyFat~.,
           body_fat_Decorrelated_trainD[,c("BodyFat",names(modelBodyFatD$coef)[-1])])
pander::pander(Delm,add.significance.stars=TRUE)
plot(Delm)

par(op)
```

## Train a Logistic Model for Overweight Prediction

This last experiment showcases the effect of data transformation on
logistic modeling. This experiment starts by creating a data-frame that
does not includes the $BMI$, $Height$, and $Weight$ variables. The
target outcome is to identify if the person is Overweight or normal.
(BMI\>=25). The next lines of code compute the new data frames and
remove the above mentioned variables.

### Data Conditioning

First Remove Height and Weight from Training and Testing Sets

```{r results = "asis",}

trainingsetBMI <- trainingset[,!(colnames(trainingset) %in% c("Weight","Height"))]
testingsetBMI <- testingset[,!(colnames(trainingset) %in% c("Weight","Height"))]
trainingsetBMI$Overweight <- 1*(trainingsetBMI$BMI>=25)
testingsetBMI$Overweight <- 1*(testingsetBMI$BMI>=25)
trainingsetBMI$BMI <- NULL
testingsetBMI$BMI <- NULL

# The number of subjects
pander::pander(table(trainingsetBMI$Overweight))
pander::pander(table(testingsetBMI$Overweight))

## The outcome-blind transformation
OW_Decorrelated_train <- ILAA(trainingsetBMI,
                              thr=0.2,
                              Outcome="Overweight",
                              verbose=TRUE)

OW_Decorrelated_test <- predictDecorrelate(OW_Decorrelated_train,testingsetBMI)

## The outcome-driven transformation

OW_Decorrelated_trainD <- ILAA(trainingsetBMI,
                               thr=0.2,
                               Outcome="Overweight",
                               drivingFeatures="Overweight",
                               verbose=TRUE)

OW_Decorrelated_testD <- predictDecorrelate(OW_Decorrelated_trainD,testingsetBMI)



```

The last code snippet transforms the observed features using ILLA and
setting a target variable and setting the convergence not to be affected
by the target outcome.

### The Logistic Model

LASSO_1SE with a binomial family is used to compute the logistic model
of overweight.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

## Outcome-blind
modelOverweight <- LASSO_1SE(Overweight~.,
                             OW_Decorrelated_train,
                             family="binomial")
pander::pander(as.matrix(modelOverweight$coef))

## Outcome-driven
modelOverweightD <- LASSO_1SE(Overweight~.,
                              OW_Decorrelated_trainD,
                              family="binomial")
pander::pander(as.matrix(modelOverweightD$coef))

```

### The Model Coefficients in the Observed Space

Once the logistic model is created in the transformed space, we can
compute the beta coefficients for each one of the observed variables.

```{r  results = "asis", warning = FALSE}

# Get the coefficients in the observed space
observedCoef <- getObservedCoef(OW_Decorrelated_train,modelOverweight)
pander::pander(as.matrix(observedCoef$coefficients))


```

### Predict Using the Transformed Data Set

The predictions of the testing set can be done using the handy
`predict()` function. The evaluation of the testing results can be
evaluated using the `predictionStats_binary()` function.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

## Outcome-blind
predicOverweight <- predict(modelOverweight,OW_Decorrelated_test)
pr <- predictionStats_binary(cbind(OW_Decorrelated_test$Overweight,
                                   predicOverweight),"Overweight: Blind")
pander::pander(pr$ClassMetrics)

## Outcome-Driven
predicOverweightD <- predict(modelOverweightD,OW_Decorrelated_testD)
pr <- predictionStats_binary(cbind(OW_Decorrelated_test$Overweight,
                                   predicOverweightD),"Overweight: Driven")
pander::pander(pr$ClassMetrics)

```

### Prediction Using the Observed Features

The predict of the testing set can be done using the `model.matrix()`
and the dot product `%*%.`

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

predicOverweightObst <- model.matrix(formula(observedCoef$formula),testingsetBMI) %*% observedCoef$coefficients
#predicOverweightObst <- 1.0/(1.0 + exp(-predicOverweightObst));

plot(predicOverweightObst,predicOverweight,
     xlab="Observed",
     ylab="Transformed",
     main="Test predictions: Observed vs. Transformed")

```

The last plot shows the expected result: both predictions are identical.

### Comparison to Raw Model

To showcase the advantage of transformed modeling *vs.* raw modeling,
here I'll estimate the logistic model from the observed variables and
contrast to the model generated from the transformed space.

The next lines of code compute the logistic model and display its
testing performance:

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}
##Training
rawmodelOverweight <- LASSO_1SE(Overweight~.,
                                trainingsetBMI,
                                family="binomial")
pander::pander(rawmodelOverweight$coef)
## Predict
rawpredicOverweight <- predict(rawmodelOverweight,testingsetBMI)
pr <- predictionStats_binary(cbind(testingsetBMI$Overweight,
                                   rawpredicOverweight),"Overweight")
pander::pander(pr$ClassMetrics)

```

The model created from the observed data has an ROC AUC that is not
statistically significant to the transformed model

### Comparing the Feature Significance on the Models

This last lines of code will compute the significance of the beta
coefficients for both the observed model and the latent-based model. The
user can clearly see that all the betas of the latent-based model are
statically significant. An effect that is not seen in the logistic
observed model.

```{r results = "asis", warning = FALSE, dpi=300, fig.height= 5.0, fig.width= 7.0}

par(mfrow=c(2,2),cex=0.5)

## Raw model
rawlm <- lm(Overweight~.,trainingsetBMI[,c("Overweight",names(rawmodelOverweight$coef)[-1])])
pander::pander(rawlm,add.significance.stars=TRUE)
plot(rawlm)

## Outcome-blind
par(mfrow=c(2,2),cex=0.5)
Delm <- lm(Overweight~.,OW_Decorrelated_test[,c("Overweight",names(modelOverweight$coef)[-1])])
pander::pander(Delm,add.significance.stars=TRUE)
plot(Delm)


## Outcome-Driven
par(mfrow=c(2,2),cex=0.5)
Delm <- lm(Overweight~.,OW_Decorrelated_testD[,c("Overweight",names(modelOverweightD$coef)[-1])])
pander::pander(Delm,add.significance.stars=TRUE)
plot(Delm)

```

# Conclusion

In conclusion, ILAA (Iterative Linear Association Analysis), stands as
an unsupervised computer-based methodology adept at estimating linear
transformation matrices. These matrices enable the conversion of
datasets into a fresh latent-based space, offering a user-controlled
degree of correlation. This report has effectively demonstrated the
practical application of ILAA, providing comprehensive insights into its
functions for estimating, predicting, and scrutinizing transformations.
Such capabilities hold significant promise in supervised learning
scenarios, encompassing regression and logistic models.
